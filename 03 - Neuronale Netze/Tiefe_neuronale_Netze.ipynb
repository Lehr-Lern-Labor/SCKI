{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze\n",
    "\n",
    "In der letzten Einheit haben wir das Perzeptron kennen gelernt, das durch einen aus Fehlern lernenden Algorithmus in bestimmten Szenarien Daten richtig klassifizieren kann. Der Klassifikationsalgorithmus des Perzeptrons stößt allerdings schnell an seine Grenzen. In dieser Einheit schauen wir uns an, wie wir das Perzeptron schrittweise verbessern können. Diese Verbesserungen führen uns zu neuronalen Netzen, die die rasante Entwicklung der KI der letzten Jahre entscheidend prägten.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/artificial-intelligence.jpg\" alt=\"Deep Neural Network\" style=\"width:50%\">\n",
    "    &nbsp;\n",
    "  <figcaption><i>KI wird in den nächsten Jahren immer mehr Aufgaben übernehmen, die jetzt noch von Menschen ausgeführt werden. Gleichzeitig schafft KI auch neue Tätigkeiten und Berufe für Menschen. Eine wichtige Herausforderung der Zukuft ist u.a. die Gestaltung einer sinnvollen Zusammenarbeit zwischen Mensch und KI. Examplarisch dafür wurde das Bild von einer KI erzeugt.</i></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Zum Einstieg in diese Einheit rufen wir uns den Aufbau des Perzeptrons in Erinnerung. Das Perzeptron besteht aus einer bestimmten Anzahl von Inputs (abhängig von den Dimensionen der Punkte, die als Datengrundlage dienen), Gewichten mit denen die Eingaben mulipliziert und zusammen mit dem Bias addiert werden und einer Aktivierungsfunktion. Diesen Aufbau bezeichnen wir im Folgenden als <b>Neuron</b>.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/perzeptron.png\" alt=\"perzeptron\" style=\"width:70%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Aufbau neuronaler Netze\n",
    "\n",
    "Im Folgenden ändern wir das Perzeptron Schritt für Schritt ab, um dessen Defizite zu beheben.\n",
    "\n",
    "### Mehr als zwei Klassen klassifizieren und Performance steigern\n",
    "\n",
    "Um die Performance unserer KI zu steigern, schalten wir mehrere Neuronen hinter- und nebeneinander. Die Ausgabe eines Neurons dient nun als Eingabe von nachfolgenden Neuronen. Sind Neuronen parallel in einer Ebene angeordnet, wird die Gesamtheit dieser Neuronen als <b>Layer</b> (bzw. Schicht) bezeichnet. Das gesamte Konstrukt mehreren Neuronenschichten bezeichnet man als <b>neuronales Netz</b>. Wenn es mehrere verdeckte Schichten gibt, bezeichnet man das Netz als <b>tiefes neuronales Netz</b> (deep neural network).\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn1.png\" alt=\"perzeptron\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Um nicht nur zwei Klassen von Datenpunkten klassifizieren zu können, wird die Ausgabe durch mehrere Neuronen erweitert. Die Nummer des Neurons, das den größten Wert in der Ausgabeschicht ausgibt, ist auch die Ausgabe des gesamten neuronalen Netzes. Wenn es also fünf Ausgabeneuronen gibt und das dritte den größten Wert hat, dann weist das neuronale Netz den Datenpunkt der Klasse 2 zu (weil es bei 0 zu zählen beginnt). Bisher sind die Ausgaben der Neuronen allerdings entweder 0 oder 1, so dass es oft zu einem Gleichstand kommen kann. Nicht nur deswegen sollten wir die bisherige Aktivierungsfunktion durch eine geeignetere ersetzen.\n",
    "\n",
    "### Neue Aktivierungsfunktion\n",
    "\n",
    "Das Perzeptron kann nur dann Datenpunkte von Klassen voneinander trennen, wenn die Datenpunkte der unterschiedlichen Klassen durch eine Gerade getrennt werden können. Wenn die Trennung der beiden Klasse eine Kurve darstellt, kann das Perzeptron die Datenpunkte nicht mehr richtig trennen. Das verursacht die Treppenfunktion, die wir als Aktivierungsfunktion verwenden. Außerdem gehen durch die Weiterleitung von entweder 0 oder 1 viele Informationen verloren. Die <b>Sigmoidfunktion</b> $sig$ oder <b>ReLU-Funktion</b> $relu$ eignen sich als Aktivierungsfunktionen der Neuronen besser. Für unsere neuronalen Netze werden wir hauptsächlich die ReLU-Funktion verwenden.\n",
    "\n",
    "$$ sig(x) = \\dfrac{e^x}{e^x + 1} $$\n",
    "\n",
    "\n",
    "$$ relu(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0, & x \\leq 0 \\\\\n",
    "x, & \\, \\textrm{sonst} \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/sigmoid_and_relu.png\" alt=\"Sigmoid and ReLU\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "### Softmax\n",
    "\n",
    "Jetzt fehlt nur noch eine kleine Änderung, um ein herkömmliches neuronales Netz zu erhalten. Wie im vorletzten Abschnitt bereits umrissen, wird die Klassifikation des Datenpunkts jetzt nicht mehr durch eine 0- oder 1-Ausgabe des letzten Neurons ermittelt, sondern durch die Nummer des Neurons in der Ausgabeschicht, der die größte Ausgabe hat. Durch die neuen ReLU-Aktivierungsfunktion erhalten wir in der letzten Ausgabeschicht nicht mehr 0- oder 1-Ausgaben, sondern Zahlen größer oder gleich 0. Um prozentuale Angabe zu erhalten, mit welcher Wahrscheinlichkeit der Datenpunkt der Klasse 0, 1, etc. zugeordnet wird, wird eine zusätzliche Schicht mit einer speziellen Aktivierungsfunktion (Softmax-Funktion) als Letztes eingefügt deren Gewichte nicht trainiert werden.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn2.png\" alt=\"perzeptron\" style=\"width:80%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Jetzt sind wir bereit unser erstes neuronales Netz in Code umzusetzen. Damit wir nicht alles selbst implementieren müssen, verwenden wir die Bibliothek <i>PyTorch</i>, die von einem Facebook-Forschungsteam entwickelt wurde.\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "PyTorch bietet eine sehr einfache Weise, neuronale Netze zu konstruieren. Gehe das folgende Codefeld durch und führe es aus, um mit den Funktionsaufrufen vertraut zu werden. Wir konstruieren dabei das letzte neuronale Netze mit vier Eingabe- und drei Ausgabeneuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo mein Name ist Netzi!\n",
      "\n",
      "Das ist mein Aufbau:\n",
      "\n",
      "Net(\n",
      "  (fc1): Linear(in_features=4, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (fc3): Linear(in_features=5, out_features=3, bias=False)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=None)\n",
      ") \n",
      "\n",
      "Und das sind meine zufällig initialisierten Gewichte:\n",
      "\n",
      " ('fc1.weight', Parameter containing:\n",
      "tensor([[-0.0706,  0.1153,  0.1311, -0.0348],\n",
      "        [-0.4804,  0.4371, -0.3316,  0.0306],\n",
      "        [ 0.2597, -0.2974,  0.0902,  0.0191],\n",
      "        [ 0.4429, -0.2629, -0.0889, -0.3075],\n",
      "        [-0.3264,  0.3375, -0.0610,  0.4415]], requires_grad=True))\n",
      "\n",
      " ('fc1.bias', Parameter containing:\n",
      "tensor([-0.0403,  0.2459, -0.2174, -0.4649, -0.4248], requires_grad=True))\n",
      "\n",
      " ('fc2.weight', Parameter containing:\n",
      "tensor([[-0.3981, -0.3841, -0.1396, -0.1453, -0.3606],\n",
      "        [ 0.3921, -0.1388, -0.4031, -0.0382, -0.2481],\n",
      "        [ 0.2809, -0.1418,  0.3626,  0.2498, -0.3887],\n",
      "        [ 0.0502,  0.2189, -0.1556, -0.2209,  0.3776],\n",
      "        [-0.3441,  0.3267,  0.0269, -0.0164, -0.3795]], requires_grad=True))\n",
      "\n",
      " ('fc2.bias', Parameter containing:\n",
      "tensor([-0.1339,  0.0481, -0.4090, -0.2080, -0.1248], requires_grad=True))\n",
      "\n",
      " ('fc3.weight', Parameter containing:\n",
      "tensor([[ 0.3267,  0.3092, -0.4253,  0.4353, -0.4394],\n",
      "        [ 0.0218,  0.0925,  0.1867,  0.3655,  0.3061],\n",
      "        [ 0.1079,  0.4165, -0.1084, -0.2972, -0.0852]], requires_grad=True))\n",
      "\n",
      "Ausgabe: tensor([0.3197, 0.3654, 0.3149], grad_fn=<SoftmaxBackward0>)\n",
      "Ausgabe: tensor([0.3197, 0.3654, 0.3149], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # In dem Konstruktor werden die unterschiedlichen Schichten definiert\n",
    "    def __init__(self, num_in, num_out):\n",
    "        \n",
    "        # Der Konstruktur der Elternklasse muss aufgerufen werden.\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.name_model = \"Netzi\"\n",
    "        \n",
    "        # Durch den folgenden Funktionsaufruf wird eine Schicht mit num_in eingehenden \n",
    "        # und 5 ausgehenden Verbindungen konstruiert.\n",
    "        # Den Namen der Schichten kannst du selbst festlegen.\n",
    "        # fc steht für fully connected.\n",
    "        self.fc1 = nn.Linear(num_in, 5)\n",
    "        # Achte darauf, dass die folgende Schicht die Anzahl der eingehenden Verbindungen\n",
    "        # mit den ausgehenden Verbindungen der letzten Schicht übereinstimmt.\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        # Standardmäßig werden zu jedem Neuron ein Bias hinzugefügt. Durch den Parameter\n",
    "        # 'bias' kann das deaktiviert werden.\n",
    "        self.fc3 = nn.Linear(5, num_out, bias=False)\n",
    "        \n",
    "        # ReLU-Funktion\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        # Softmax-Funktion\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    # In dieser Funktion muss festgelegt werden, wie die Eingabe durch das Netz propagiert wird (d.h. durch\n",
    "    # die einzelnen Schichten „weitergereicht“ wird).\n",
    "    def forward(self, x):\n",
    "        # Zunächst wird die Eingabe mit den Gewichten der ersten Schicht multipliziert, \n",
    "        # in den einzelnen Neuronen aufsummiert und anschließend in die ReLU-Funktion eingesetzt.\n",
    "        output = self.relu(self.fc1(x))\n",
    "        # Die verarbeitete Eingabe wird nun durch die zweite Schicht propagiert. \n",
    "        output = self.relu(self.fc2(output))\n",
    "        # In der vorletzten Schicht gibt es keine ReLU-Funktion mehr.\n",
    "        output = self.fc3(output)\n",
    "        # finale Ausgabe des neuronalen Netzes\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "# Erzeugung eines Objekts des neuronalen Netzes\n",
    "erstes_nn = Net(4,3)\n",
    "print(f\"Hallo mein Name ist {erstes_nn.name_model}!\\n\")\n",
    "print(\"Das ist mein Aufbau:\\n\")\n",
    "print(erstes_nn, \"\\n\")\n",
    "print(\"Und das sind meine zufällig initialisierten Gewichte:\")\n",
    "for param in erstes_nn.named_parameters():\n",
    "    print(\"\\n\", param)\n",
    "\n",
    "# Das ist eine Testeingabe\n",
    "test_eingabe = torch.tensor([1.0, 2.5, -1, 0])\n",
    "# Die Ausgabe erhälst du entweder so\n",
    "print(\"\\nAusgabe:\", erstes_nn(test_eingabe))\n",
    "# oder durch den Funktionsaufruf forward(eingabe)\n",
    "print(\"Ausgabe:\", erstes_nn.forward(test_eingabe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Im letzten Codefeld wurden unser erstes neuronales Netz erzeugt. Lese die gesuchten Gewichte anhand der letzten Ausgabe ab und überprüfe deine Eingabe, indem du das Codefeld ausführst. Runde gegebenenfalls die Eingaben auf die vierte Nachkommastelle ab.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das erste Gewicht hast du nicht richtig abgelesen!\n",
      "Das zweite Gewicht hast du nicht richtig abgelesen!\n",
      "Das dritte Gewicht hast du nicht richtig abgelesen!\n",
      "Das vierte Gewicht hast du nicht richtig abgelesen!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from resources.code.help_functions import pruefe_gewichte\n",
    "\n",
    "# Ersetze die Nullen durch die richtigen Werte.\n",
    "\n",
    "# Gewicht zwischen dem ersten Neuron der Eingabeschicht und dem ersten Neuron der ersten verdeckten Schicht\n",
    "gewicht1 = 0\n",
    "\n",
    "# Bias des letzen Neurons der ersten verdeckten Schicht\n",
    "gewicht2 = 0\n",
    "\n",
    "# Bias des zweiten Neurons der zweiten verdeckten Schicht\n",
    "gewicht3 = 0\n",
    "\n",
    "# Gewicht zwischen dem vierten Neuron der zweiten verdeckten Schicht \n",
    "# und dem dritten Neuron der dritten verdeckten Schicht\n",
    "gewicht4 = 0\n",
    "\n",
    "print(pruefe_gewichte(erstes_nn, gewicht1, gewicht2, gewicht3, gewicht4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Jetzt bist du bereit ein neuronales Netz eigenständig zu konstruieren. Implementiere das abgebildete neuronale Netz und gebe das Ergebnis des durchpropagierten Datenpunkts an.</i>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn3.png\" alt=\"neuronales Netz\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datenpunkt = tensor([1.0, 2.0])\n",
    "\n",
    "# Füge hier deinen Code ein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt haben wir zwar neuronale Netze konstruiert, aber sie noch nicht trainieren lassen. Die vorhandenen Trainingsdaten müssen wir nutzen, um die Gewichte so so anzupassen, dass das neuronale Netz auf den Testdaten (die wir nicht für das Training benutzen) gute Ergebnisse erzielt. Im nächsten Abschnitt schauen wir uns, wie das funktioniert.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Der Algorithmus, der die Gewichte der neuronalen Netze abändert und ein entscheidender Faktor am Erfolg von Deep-Learning-Algorithmen ist, ist der <b>Backpropagation-Algorithmus</b>. Der Backpropagation-Algorithmus ist ein Optimierungsalgorithmus, d.h. bei der Funktion, die den Fehler des neuronalen Netzes beschreibt, wird (in diesem Fall) nach dem Minimum gesucht, weil wir den Fehler so klein wie möglich halten möchten. \n",
    "\n",
    "Die Suche nach dem Minimum können wir uns mit folgendem Bild veranschaulichen. Ein Weihnachtsmann sitzt in seinem E-Schlitten auf einem Hügel und möchte den Weg ins Tal finden. Da es schon dunkel und sogar etwas nebelig ist, kann er nur zehn Meter weit sehen. Leider kennt er auch den Weg ins Tal nicht. Er weiß allerdings, dass die Ableitung an dem Punkt, an dem er sich befindet in die Richtung des Abstiegs zeigt. Er stellt sein E-Schlitten so ein, dass er eine bestimmte Distanz in die Richtung des Abstiegs fährt, anschließend stoppt, die Richtung des Abstiegs noch einmal neu bestimmt und in diese Richtung wieder eine bestimmte Distanz fährt. Wenn alles optimal verläuft, findet er auf diese Weise den Weg ins Tal.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/loss_function.png\" alt=\"Verlustfunktion\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Analog dazu funktioniert auch die Optimierung bei neuronalen Netzen. Dafür legen wir als Verlustfunktion die Funktion MSE fest:\n",
    "\n",
    "$$MSE = \\dfrac{1}{n} \\bigl[ (y_1 - o_1)^2 + (y_2 - o_2)^2 + \\dots + (y_n - o_n)^2 \\bigr], $$\n",
    "\n",
    "wobei $(y_1, \\dots, y_n)$ die optimale und $(o_1, \\dots, o_n)$ die tatsächliche Ausgabe beschreibt. \n",
    "\n",
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "    \n",
    "<i>Wenn wir z.B. einen Datenpunkt betrachten, der ein Blaumeisenei repräsentiert, dann ist die optimale Ausgabe bei drei möglichen Klassen (Klasse 0 = Blaumeisenei, Klasse 1 = Entenei, Klasse 2 = Greifvogelei) der Vektor $(1, 0, 0)$. Wenn die tatsächliche Ausgabe des neuronalen Netzes $(0.5, 0.25, 0.25)$ ist, was ist dann der Verlust nach der oberen Formel?</i>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>➤ Klicke hier, um deine Antwort zu prüfen.</summary>\n",
    "   \n",
    "$$\\dfrac{1}{3} \\bigl[ (1 - 0.5)^2 + (0 - 0.25)^2 + (0 - 0.25)^2 \\bigr] = 0.375.$$\n",
    "    \n",
    "Wenn das neuronale Netz nur ein Gewicht hat, könnte die Verlustfunktion so aussehen:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/loss_function2.png\" alt=\"Verlustfunktion\" style=\"width:45%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Das aktuelle Gewicht $w_1$ von $0.7$ muss also bisschen vergrößert werden, um den Verlust zu verkleinern.\n",
    "   \n",
    "</details>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Wenn das neuronale Netz nur zwei Gewichte hat, könnte eine Verlustfunktion wie folgt aussehen. Bei mehr als zwei Gewichten (in der Praxis eingesetzte neuronale Netze haben Millionen von trainierbaren Gewichten) ist eine Visualisierung allerdings nicht mehr so einfach möglich.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/train_val_loss_landscape.png\" alt=\"Loss-Function\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Wenn wir bestimmt haben, ob wir ein Gewicht verkleiner oder vergrößern müssen, um den Verlust zu reduzieren, müssen wir noch festlegen, wie stark wir das Gewicht verändert möchten. Dabei können unterschiedliche Probleme auftreten. Ist die Veränderung des Gewichts zu gering, kann es sein, dass das neuronale Netz in einem lokalen Minimum stecken bleibt oder sich nur sehr langsam dem globalen Minimum nähert. Verändert wir das Gewicht zu stark, ist es möglich, dass wir über das Ziel hinausschießen. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/loss_function3.png\" alt=\"Verlustfunktion\" style=\"width:95%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Wir müssen also die <b>Lernrate</b> des neuronalen Netzes mit Bedacht wählen und möglicherweise immer wieder anpassen. Die Update-Regel für jedes Gewicht $w$ im neuronalen Netz können wir folgendermaßen notieren:\n",
    "\n",
    "$$w_{\\text{neu}} \\longleftarrow w_{\\text{alt}} - \\alpha \\cdot \\Delta w.$$\n",
    "\n",
    "$\\alpha$ ist die Lernrate und $\\Delta w$ der Gradient (die Ableitung) des Gewichts. Der Gradient gibt nicht nur die Richtung an, in der das Gewicht verändert werden muss, sondern beschreibt auch, wie stark das betrachtete Gewicht zu dem Verlust beigetragen hat. \n",
    "\n",
    "Den Gradienten eines Gewichts $w$ bestimmen wir, indem wir die Verlustfunktion nach $w$ durch mehrfache Anwendung der Kettenregel ableiten. Da es dieser Prozess sehr mühselig ist, verzichten wir an dieser Stelle auf weitere Details, weil PyTorch für uns diese Arbeit übernehmen wird.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/backpropagation.png\" alt=\"Verlustfunktion\" style=\"width:65%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Training eines neuronalen Netzes\n",
    "\n",
    "Nach so viel Theorie können wir endlich neuronale Netze trainieren lassen! Untersuche den Code, um dein eigenes neuronales Netz weiter unten an die Daten anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir haben 500 Datenpunkte zur Verfügung.\n"
     ]
    }
   ],
   "source": [
    "# Das sind die Daten, die uns zur Verfügung stehen.\n",
    "\n",
    "from resources.code.help_functions import daten\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "daten, plt = daten()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Wir haben {len(daten)} Datenpunkte zur Verfügung.\")\n",
    "\n",
    "# Hier werden die Daten durchmischt.\n",
    "daten = random.sample(daten, k=len(daten))\n",
    "\n",
    "# Die Daten werden in Trainings- und Testdaten aufgeteilt (Verhältnis 8:2).\n",
    "trainings_daten = daten[:400]\n",
    "test_daten = daten[400:]\n",
    "del daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir implementieren folgendes neuronales Netz, das du bereits oben konstruiert hast.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn3.png\" alt=\"neuronales Netz\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5557, 0.4443],\n",
      "        [0.5642, 0.4358]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_in, num_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in, 2)\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, train_labels, lr):\n",
    "    model.train(True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_data)\n",
    "    loss = loss_fn(outputs, train_labels)\n",
    "    print('#####', loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "def test(model, test_data, test_labels):\n",
    "    model.train(False)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    outputs = model(test_data)\n",
    "    loss = loss_fn(outputs, test_labels)\n",
    "    correctly_classified = 0\n",
    "    for idx, x in enumerate(outputs):\n",
    "        if torch.argmax(x) == test_labels[idx]:\n",
    "            correctly_classified += 1\n",
    "    print(f\"Correctly classified: {correctly_classified} / {outputs.shape[0]}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([])\n",
    "Y_train = torch.tensor([])\n",
    "X_test = torch.tensor([])\n",
    "Y_test = torch.tensor([])\n",
    "\n",
    "for datenpunkt in trainings_daten:\n",
    "    koordinaten = torch.tensor([[datenpunkt[0][0], datenpunkt[0][1]]])\n",
    "    X_train = torch.cat( ( X_train, koordinaten), dim=0 )\n",
    "    Y_train = torch.cat( ( Y_train, torch.tensor([[datenpunkt[1]]])), dim=0 )\n",
    "    \n",
    "for datenpunkt in test_daten:\n",
    "    koordinaten = torch.tensor([[datenpunkt[0][0], datenpunkt[0][1]]])\n",
    "    X_test = torch.cat( ( X_test, koordinaten), dim=0 )\n",
    "    Y_test = torch.cat( ( Y_test, torch.tensor([[datenpunkt[1]]])), dim=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 53 / 100\n",
      "Loss=0.25020331144332886\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 53 / 100\n",
      "Loss=0.25019094347953796\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501794397830963\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501687705516815\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501588463783264\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250149667263031\n",
      "##### tensor(0.2502, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25014108419418335\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25013309717178345\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501257061958313\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25011879205703735\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501123547554016\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501063644886017\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2501008212566376\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500956058502197\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500907778739929\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500862777233124\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500820457935333\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25007811188697815\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25007447600364685\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25007107853889465\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25006788969039917\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250064879655838\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25006210803985596\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25005948543548584\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25005707144737244\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500547468662262\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500525712966919\n",
      "##### tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500506043434143\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25004875659942627\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500469982624054\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500452995300293\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25004374980926514\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500423192977905\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500408887863159\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25003957748413086\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25003835558891296\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25003722310066223\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500361204147339\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500351071357727\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500340938568115\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500331997871399\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25003230571746826\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500314712524414\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500306963920593\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002995133399963\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002920627593994\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500285506248474\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500278949737549\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002726912498474\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500267028808594\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500261068344116\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002557039260864\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002506375312805\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002455711364746\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002411007881165\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002360343933105\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500231862068176\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500227689743042\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002235174179077\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002190470695496\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500215768814087\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25002121925354004\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500208616256714\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500205338001251\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500201463699341\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500198483467102\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001952052116394\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001922249794006\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500189244747162\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500186264514923\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500183582305908\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001806020736694\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001779198646545\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001755356788635\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001728534698486\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500170171260834\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001680850982666\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500165104866028\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001633167266846\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001609325408936\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001585483551025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001564621925354\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500154376029968\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500152289867401\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500150203704834\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500148117542267\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001460313796997\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001442432403564\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500142455101013\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500140368938446\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500138580799103\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001364946365356\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001347064971924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500133216381073\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500131130218506\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001296401023865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500127851963043\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500126361846924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001248717308044\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500123083591461\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500121593475342\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001201033592224\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500118613243103\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250011682510376\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001153349876404\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500114142894745\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001126527786255\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500111162662506\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001099705696106\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500108480453491\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001072883605957\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001057982444763\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500104308128357\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001028180122375\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500101327896118\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25001007318496704\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500099241733551\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000980496406555\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250009685754776\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500095069408417\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500094175338745\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000935792922974\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500092089176178\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000903010368347\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500089704990387\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000882148742676\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500087320804596\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000864267349243\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500084936618805\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000840425491333\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500082850456238\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500081956386566\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000810623168945\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500079572200775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000789761543274\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500077784061432\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500077188014984\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000759959220886\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500074803829193\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000742077827454\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500073313713074\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500072419643402\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000712275505066\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500070631504059\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000694394111633\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000688433647156\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250006765127182\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000670552253723\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500065863132477\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500065565109253\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000643730163574\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000637769699097\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500063180923462\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000622868537903\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000613927841187\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500060796737671\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000596046447754\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000590085983276\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250005841255188\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500057816505432\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000569224357605\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500056028366089\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500055432319641\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000548362731934\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500053942203522\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500053346157074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500052750110626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000518560409546\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500051259994507\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500051259994507\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000500679016113\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000494718551636\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000491738319397\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500048577785492\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000476837158203\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000467896461487\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000467896461487\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500045895576477\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000452995300293\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000444054603577\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500044107437134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500043511390686\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500043213367462\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000426173210144\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000420212745667\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500041723251343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500041127204895\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000402331352234\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000399351119995\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500039339065552\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500039041042328\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250003844499588\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500038146972656\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000378489494324\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500036954879761\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500036358833313\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500036060810089\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000348687171936\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250003457069397\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500034272670746\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500033676624298\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500033676624298\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500033378601074\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000327825546265\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000321865081787\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000321865081787\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500031590461731\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500031590461731\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000306963920593\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000303983688354\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000298023223877\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500029504299164\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500029504299164\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500028908252716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000283122062683\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000280141830444\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000277161598206\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000274181365967\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500027120113373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500027120113373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500026524066925\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000259280204773\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000256299972534\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000256299972534\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000250339508057\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500024735927582\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500024735927582\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500024437904358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000235438346863\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000235438346863\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000232458114624\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000229477882385\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000229477882385\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000226497650146\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000226497650146\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500021755695343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500021755695343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500021457672119\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500021159648895\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000205636024475\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000205636024475\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000202655792236\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500019967556\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500019669532776\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500019371509552\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500019073486328\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500019073486328\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500018775463104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500018775463104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500018775463104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000178813934326\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000178813934326\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500017285346985\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500017583370209\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500016987323761\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500016987323761\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500016689300537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500016689300537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000160932540894\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500016391277313\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000160932540894\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000154972076416\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000154972076416\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500015199184418\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500015199184418\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250001460313797\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250001460313797\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250001460313797\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250001460313797\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500014007091522\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000137090682983\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500014007091522\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500014007091522\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000137090682983\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000134110450745\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000134110450745\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000131130218506\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000128149986267\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000131130218506\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500012516975403\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500012516975403\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500012218952179\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500012218952179\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000113248825073\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000110268592834\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000113248825073\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000107288360596\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000107288360596\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000110268592834\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000104308128357\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000104308128357\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000104308128357\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500010132789612\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500009834766388\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500009834766388\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500009834766388\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500009834766388\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500009536743164\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250000923871994\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified: 0 / 100\n",
      "Loss=0.250000923871994\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000089406967163\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000086426734924\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000083446502686\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000083446502686\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000080466270447\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000080466270447\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000080466270447\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000080466270447\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007748603821\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007450580597\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007748603821\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007152557373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007152557373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007450580597\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007152557373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007450580597\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007152557373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500006854534149\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500006854534149\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500007152557373\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000062584877014\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500006854534149\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000065565109253\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000065565109253\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000065565109253\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000059604644775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000065565109253\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000062584877014\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000059604644775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000059604644775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000059604644775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000059604644775\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000056624412537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000056624412537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000056624412537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250000536441803\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000056624412537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.250000536441803\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000056624412537\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004768371582\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004470348358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004470348358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500005066394806\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004470348358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004470348358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004172325134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004172325134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004172325134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004172325134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004470348358\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500004172325134\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000038743019104\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000032782554626\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000035762786865\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002682209015\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002682209015\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002980232239\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002682209015\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002086162567\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002086162567\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002086162567\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002384185791\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500002086162567\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500001788139343\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000014901161194\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500000596046448\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000011920928955\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.25000008940696716\n",
      "##### tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "Correctly classified: 0 / 100\n",
      "Loss=0.2500000596046448\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def get_model(model):\n",
    "      return deepcopy(model)\n",
    "    \n",
    "epochs = 500\n",
    "net = Net(2,2)\n",
    "lr = 0.1\n",
    "loss_min = 100000000\n",
    "counter = 0\n",
    "best_model = get_model(net)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # Training\n",
    "    train(net, X_train, Y_train, lr)\n",
    "    # Test\n",
    "    loss = test(net, X_test, Y_test)\n",
    "    print(f\"Loss={loss}\")\n",
    "    \n",
    "    # Falls der Loss nach drei Durchläufen nicht wesentlich kleiner wird \n",
    "    # als der bisher minimaler Loss, dann wird die Lernrate um den Faktor \n",
    "    # 10 verkleinert.\n",
    "    \n",
    "    if loss - loss_min > 0.1:\n",
    "        print(\"Diff=\", loss - loss_min)\n",
    "        counter += 1\n",
    "        if counter == 3:\n",
    "            lr /= 10\n",
    "            counter = 0\n",
    "            model = best_model\n",
    "            print(\"#### New learning rate\", lr)\n",
    "            \n",
    "    # Wenn der aktuelle Loss minimal ist, dann wird dieser in der Variablen \n",
    "    # 'loss_min' gespeichert und das beste Modell in der Variablen 'best_model'.\n",
    "    \n",
    "    else:\n",
    "        loss_min = loss\n",
    "        counter = 0\n",
    "        best_model = get_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Führe dieses Feld aus, indem du entweder oben auf 'Run' klickst oder 'Strg + Enter' drückst.\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Importiere zunächst alle notwendigen Bibliotheken für dieses Jupyter Notebook, indem du das untere Codefeld ausführst. Beachte, dass du bei jedem Neustart des Kernels auch das untere Codefeld jedes Mal ausführen musst.\n",
    "\n",
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "    \n",
    "<i>Wir haben gelernt, wie wir das Perzeptron trainieren können Doch warum haben wir das eigentlich gemacht?</i>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>➤ Klicke hier, um deine Antwort zu prüfen.</summary>\n",
    "   \n",
    "Wir möchten das Perzeptron so trainieren, dass wir <b>unbekannte</b> Daten, auf denen wir unser Perzeptron <b>nicht</b> trainiert haben, richtig klassifizieren. Die Performance auf den <b>Trainingsdaten</b> dient zwar als Orientierung, ist aber nicht besonders wichtig.\n",
    "   \n",
    "</details>\n",
    "\n",
    "<h2>Was sind tiefe neuronale Netze?</h2>\n",
    "\n",
    "Bis jetzt hast du gelernt, dass ein neuronales Netz aus einer Eingabeschicht (engl. <i>input layer</i>), einer Ausgabeschicht (engl. <i>output layer</i>) und einer Schicht zwischen der Eingabe- und Ausgabeschicht besteht. Die dazwischenliegende Schicht bezeichnet man als <i>hidden layer</i>. Hat ein neuronales Netz eine ausreichend große Anzahl an <i>hidden layers</i>, so wird es als <b>tiefes neuronales Netz</b> (engl. <i><b>deep neural network</b></i>) bezeichnet. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/deep_neural_network.webp\" alt=\"Deep Neural Network\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "In der Grafik siehst du, dass die Neuronen einer beliebigen Schicht mit allen Neuronen der nachfolgenden Schicht verbunden sind. Solche Schichten, die vollständig mit der nachfolgenden Schicht verbunden sind, bezeichnet man als <i><b>fully-connected layer</b></i>. Da dieses neuronale Netz nur aus solchen Schichten besteht, nennt man so ein Netz auch <i>Fully Connected Neural Network</i>.\n",
    "\n",
    "Im Folgenden möchten wir so ein Netz mithilfe von <i>PyTorch</i> umsetzen, um es mithilfe von selbst generierten Daten trainieren zu lassen. \n",
    "\n",
    "____\n",
    "\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Ergänze die folgende Klasse, indem du drei vollständig verbundene Schichten im Konstruktur der Klasse einfügst. Die Anzahl der Neuronen der Eingabe- und Ausgabeschichten werden dabei als Parameter angegeben. Die versteckte Schicht soll fünf Neuronen enthalten. Die Funktion 'forward' brauchst du vorerst nicht zu beachten.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Beispielschicht mit 3 Eingängen und 4 Ausgaben.\n",
    "schicht_beispiel = nn.Linear(3, 4)\n",
    "\n",
    "class Net_Relu(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_in, num_out):\n",
    "        super(Net_Relu, self).__init__()\n",
    "        # Füge hier die Layer ein.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # weiter unten\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun möchten wir einen Punkt als Eingabe in unser neuronales Netz einsetzen und eine Ausgabe bekommen. Dafür müssen wir in unserem Netz die Methode <i>forward</i> implementieren. In dieser Methode geben wir an, wie die Eingabe Layer für Layer verarbeitet wird.\n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Implementiere oben in deinem neuronalen Netz die Methode 'forward'. Die Eingabe des ersten Layers entspricht dabei dem Parameter 'num_in', die Eingabe des zweiten Layers der Ausgabe des ersten Layers usw. Die Methode soll die Ausgabe der letzten Schicht in Prozent zurückgeben. (Du brauchst zunächst keine Aktivierungsfunktion zu verwenden.)</i>\n",
    "\n",
    "<h3>Trainieren und testen</h3>\n",
    "\n",
    "Jetzt sind wir bereit unser neuronales Netz zu trainieren. Wir haben bereits oben unsere Trainingsdaten erzeugt. Jetzt müssen wir diesen Daten noch labeln, d.h. der entsprechenden Klasse zuweisen. Dieser Prozess ist normalerweise sehr aufwendig, weil ein oder sogar mehrere Menschen sich die Daten durchsehen und der entsprechenden Kategorie zuweisen müssen (du hast das bestimmt auch schon einmal unfreiwillig bei einer Captcha gemacht). Für unseren Fall können wir aber für das Labeln die uns bekannte Funktion benutzen, die das neuronale Netz erlernen soll. \n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Erzeuge einen Tensor, dessen Einträge die entsprechenden Labels der Datenpunkte sind.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben also nun Trainingsdaten erzeugt. Um ein zufriedenstellendes Training zu gewährleisten, brauchen wir allerdings noch Testdaten, die sich jeweils von den Trainingsdaten <b>unterscheiden</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Erzeuge 50 Testdatenpunkte und label sie entsprechend der Funktion wie bei den Trainingsdaten.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa-solid fa-eye\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Versuche die Idee der Implementierung der Trainingsmethode zu verstehen. Erzeuge eine Instanz von deinem neuronalen Netz und übergebe der Trainingsmethode dein Modell. Lass dir dabei mit der 'print'-Funktion die Ausgaben des neuronalen Netzes und die Labels der Datenpunkte ausgeben, um besser nachzuvollziehen, wie das Training abläuft.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, train_labels):\n",
    "    model.train(True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    for i in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_data)\n",
    "        loss = loss_fn(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu beurteilen, wie gut das neuronale Netz lernt, betrachten wir die <b>Loss-Funktion</b>. Die Loss-Funktion gibt uns an, wie weit die Ausgaben des neuronalen Netzes von den richtigen Labels abweichen. Je kleiner der entsprechende Wert der Loss-Funktion für ein bestimmte Eingabe, desto besser ist die Klassifikation.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/train_val_loss_landscape.png\" alt=\"Loss-Function\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Ein weiteres Maß zur Bewertung der Güte eines neuronalen Netzes ist die <i><b>Accuracy</b></i>. Die Accuracy gibt in Prozent an, wie viele Daten richtig klassifiziert wurden. \n",
    "    \n",
    "$\\text{Accuracy} = \\frac{\\text{correctly classified}}{\\text{total}} $\n",
    "\n",
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "<i>Auf welchen Daten muss die Loss-Function und die Accuracy möglichst gut sein? Auf den Trainings- oder Testdaten? Ist die Accuracy immer ein gutes Maß, um die Güte eines neuronalen Netzes zu beurteilen?</i>\n",
    "\n",
    "&nbsp;\n",
    "<details>\n",
    "<summary>➤ Klick hier, um deine Antwort zu prüfen.</summary>\n",
    "Das Ziel ist es, ein neuronales Netz so zu trainieren, dass es ungesehenen Daten gut klassifizieren kann. Ein geringer Wert der Loss-Funktion und eine hohe Accuracy sind zwar Indizien dafür, dass das neuronale Netz etwas gelernt hat, aber nicht ausreichend dafür, dass es ungesehenen Daten gut klassifiziert. Es kann nämlich sein, dass das Netz die Trainingsdaten auswendig gelernt hat und die Perfomance auf unbekannten Daten sehr schlecht ist.\n",
    "    \n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/overfitting.png\" alt=\"Overfitting\" style=\"width:40%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "Es ist also wichtig, das Training rechtzeitig abzubrechen.\n",
    "    \n",
    "</details>\n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Implementiere eine Testfunktion, die den Wert der Loss-Funktion und der Accuracy auf den Testdaten zurückgibt.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Implementiere nun den gesamten Trainingsprozess. Werte dabei nach jeder Epoche die Loss-Funktion und die Accuracy auf den Testdaten aus.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Ersetze nun deine Funktion durch eine nicht-lineare Funktion (z.B.$f(x) = \\frac{1}{2} x^2 - 20$) und lass das Netzwerk noch einmal trainieren. Was beobachtest du?</i>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<details>\n",
    "<summary>➤ Klick hier, um fortzufahren.</summary>\n",
    "\n",
    "Das Netzwerk sollte die Daten nicht mehr mit so einer guten Genauigkeit klassifizieren können, weil die Daten nicht mehr mit Hilfe einer Linie trennbar sind. Das liegt daran, dass keine Aktivierungsfunktion verwendet wird bzw. die Aktivierungsfunktion jeder Schicht des neuronalen Netzes linear ist. \n",
    "    \n",
    "</details>\n",
    "\n",
    "<h3>Was lernt eigentlich das neuronale Netzwerk beim Training?</h3>\n",
    "\n",
    "In diesem Abschnitt gehen wir der Frage nach, was das neuronale Netzwerk beim Training eigentlich lernt.\n",
    "\n",
    "Wie wir wissen sind unterschiedliche Daten (wie z.B. Bilder) n-dimensionale Punkte in einem n-dimensionalem Koordinatensystem. Wenn das Netzwerk nun mit diesen Punkten trainiert wird, lernt es im Laufe des Trainings, welche Punkte zu welcher Klasse gehören und kann im Optimalfall auch ähnliche unbekannte Daten richtig einordnen.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/layer_sizes.jpeg\" alt=\"2-dimensional space\" style=\"width:25%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<b>Was das Netzwerk also im Grunde genommen lernt, ist eine Trennlinie/-kurve zwischen den Datenpunkten zu ziehen</b>. \n",
    "\n",
    "Wenn die Aktivierungsfunktion linear ist, so ist es auch nur möglich linear trennbare Datenpunkte zu separieren. Wenn die Aktvierungsfunktion nicht linear ist, so ist es auch möglich wie in der Grafik Daten mithilfe einer Trennkurve voneinander zu trennen.\n",
    "\n",
    "Die wahrscheinlich am häufigsten verwendete nicht lineare Funktion in neuronalen Netzen ist die ReLu-Funktion. \n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Füge in der 'forward'-Methode deines neuronalen Netzes nach jeder Ausgabe eines Layers eine ReLu-Funktion hinzu und lasse das Netzwerk erneut auf den Daten, die durch deine nicht-lineare Funktion erzeugt wurden trainieren. Was beobachtest du nun?</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Zusatzaufgabe: Generiere drei oder mehr Klassen von Datenpunkten mit Hilfe einer von dir gewählten Funktion. Trainiere anschließend ein neuronales Netz, das neue, frei gewählte Datenpunkte (der Funktion entsprechend) deinen gewählten Klassen richtig zuordnet.</i>\n",
    "\n",
    "<span style=\"color:blue\"><i>Übrigens</i></span>: Die Berechnung der Gradienten bei der Backpropagation erfordern sehr viel Rechenaufwand. Eine CPU wird nur bei kleinen Daten(mengen) gute Ergebnisse in überschaubarer Zeit liefern können. Aus diesem Grund verwendet man GPU-Einheiten (Grafikprozessoren), um ein neuronales Netz trainieren zu lassen. Der Vorteil dieser Verwendung besteht darin, dass die Berechnungen <i>parallel</i> ablaufen können und das Netz somit viel schneller trainiert.\n",
    "\n",
    "<h2>Convolutionale Neural Networks (CNNs)</h2>\n",
    "\n",
    "Angenommen du möchtest (relativ große) Bilder mit einem Netzwerk aus Fully-Connected-Layern klassifizieren. \n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "<i>Eine Schicht hat 64 Neuronen und ist mit einer Schicht aus 100 Neuronen verbunden. Wie viele Gewichte werden zwischen diesen beiden Schichten gelernt?</i>\n",
    "\n",
    "&nbsp;\n",
    "<details>\n",
    "<summary>➤ Klick hier, um deine Antwort zu prüfen.</summary>\n",
    "64 * 100 = 6400 Gewichte (ohne die Bias-Gewichte) werden gelernt.\n",
    "    \n",
    "</details>\n",
    "\n",
    "Die Anzahl der erlernbaren Gewichten kann in einem Fully-Connected-Netzwerk sehr groß werden und das Lernen u.a. deshalb ineffektiv. Aus diesem Grund hat man sich für die Klassifikation von Bidern eine neue Architektur von neuronalen Netzen überlegt.\n",
    "\n",
    "<b><i>Convolutional Neural Networks</i></b> (deutsch faltende neuronale Netze) bestehen aus unterschiedlichen Schichten. \n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/cnn.jpeg\" alt=\"CNN\" style=\"width:70%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa-solid fa-eye\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Schau dir folgendens Video an, um die Idee von CNNs zu verstehen:</i> https://www.youtube.com/watch?v=YRhxdVk_sIs\n",
    "\n",
    "Die <b>Convolutional Layer</b> bilden den Kern von CNNs. In der Grafik siehst du, was genau in so einem Layer passiert.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/convlayer.png\" alt=\"CNN\" style=\"width:70%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "(Hier ist das Prinziep des Convolutional Layers mit einer Animation dargestellt: https://developers.google.com/machine-learning/glossary/#convolutional_neural_network)\n",
    "\n",
    "Nach einem Convolutional Layer folgt zumeist ein <b>Max Pooling Layer</b>, der die Informationen reduziert. Die Gewichte von dieser Schicht werden <i>nicht</i> erlernt.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    " <figure>\n",
    "  <img src=\"img/max_pooling.png\" alt=\"Max Pooling\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Implementiere ein Convolutional Neural Network, um die Klassifikation von den gemalten Ostereiern zu umzusetzen. Du findest im Folgenden kleine Codesnippets, die dir helfen sollen diese Aufgabe zu lösen. Zögere nicht, dir von jemandem helfen zu lassen!</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier könnte der Pfad zu deinen Daten / die Pfade zu den Ordnern mit den Trainings- und Testbildern stehen.\n",
    "train_dataset_path = 'DataSet/train'\n",
    "test_dataset_path = 'DataSet/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier könnte die Klasse deines CNNs implementiert sein.\n",
    "# Folgende Methoden könnten dir dabei helfen.\n",
    "\n",
    "# Convolutional Layer\n",
    "intput_channels = 3\n",
    "output_channels = 6\n",
    "kernel_size = 5\n",
    "nn.Conv2d(intput_channels, output_channels, kernel_size)\n",
    "\n",
    "# Max Pooling Layer\n",
    "kernel_size_max = 2\n",
    "nn.MaxPool2d(kernel_size_max, 1)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        # print(x.shape)\n",
    "        # x = x.view(-1, 16 * 617 * 433)            # -> n, 400\n",
    "        # x = x.view(-1, 16 * 4 * 4)\n",
    "        x = x.view(-1, 16 * 13 * 13)\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0aa74ec99aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hier werden deine Trainings- und Testdaten in einem Trainloader geladen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_transforms = transforms.Compose([\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Hier werden deine Trainings- und Testdaten in einem Trainloader geladen.\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "  transforms.Resize([64,64]),\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=train_transforms)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-54e64aed2483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Verwende diesen Optimizer für ein neuronales Netz 'net'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Hier könntest du deine train- und test-Methoden implementieren.\n",
    "# Orientiere dich am oberen Code.\n",
    "\n",
    "# Verwende diesen Optimizer für ein neuronales Netz 'net'\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier könntest du dein Training umsetzen.\n",
    "# Nach jeder Epoche solltest du deinen Loss und die Accuracy ausgeben.\n",
    "# Tipp: Speichere immer das Modell mit dem geringsten durchschnittlichen \n",
    "# Loss-Wert als optimales Modell ab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit dieser Methode kannst du dir Bilder deines Trainloaders anzeigen lassen.\n",
    "# Beschrifte die Bilder zusätzlich mit einem passenden Label.\n",
    "\n",
    "for x,y in train_loader:\n",
    "    image = x[0]\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Aktuelle Forschung</h3>\n",
    "\n",
    "Die Forschung zu neuronalen Netzen ist noch lange nicht ausgeschöpft. Es gibt noch sehr vieles zu entdecken. So wissen wir z.B. immer noch nicht richtig, was genau neuronale Netze eigentlich lernen. \n",
    "\n",
    "Wenn du dich für die aktuelle Forschung zu neuronalen Netzen oder KI interessiert, ist folgender YouTube-Kanal empfehlenswert: \n",
    "\n",
    "https://www.youtube.com/c/K%C3%A1rolyZsolnai/videos?view=0&sort=p&shelf_id=0\n",
    "\n",
    "<h2>Bildquellen</h2>\n",
    "\n",
    "https://pixabay.com/de/photos/ai-generiert-junge-junger-mann-7772478/\n",
    "\n",
    "https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\n",
    "\n",
    "https://miro.medium.com/max/1386/1*bV7S0zACdidh11ikjYpLpQ.png\n",
    "\n",
    "https://pyimagesearch.com/wp-content/uploads/2019/10/train_val_loss_landscape.png\n",
    "\n",
    "https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\n",
    "\n",
    "https://anhreynolds.com/img/cnn.png\n",
    "\n",
    "https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
