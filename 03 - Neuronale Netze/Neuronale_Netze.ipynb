{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze\n",
    "\n",
    "In der letzten Einheit haben wir das Perzeptron kennen gelernt, das durch Fehler lernt und in bestimmten Szenarien Daten richtig klassifizieren kann. Der Klassifikationsalgorithmus des Perzeptrons stößt allerdings schnell an seine Grenzen. In dieser Einheit schauen wir uns an, wie wir das Perzeptron schrittweise verbessern können. Diese Verbesserungen führen uns zu neuronalen Netzen, die die rasante Entwicklung der KI der letzten Jahre entscheidend prägten.\n",
    "\n",
    "KI wird in den nächsten Jahren immer mehr Aufgaben übernehmen, die jetzt noch von Menschen ausgeführt werden. Gleichzeitig schafft KI auch neue Berufe und Perspektiven. Eine wichtige Herausforderung der Zukuft ist u.a. die Gestaltung einer sinnvollen Zusammenarbeit zwischen Mensch und KI. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/artificial-intelligence.jpg\" alt=\"Deep Neural Network\" style=\"width:50%\">\n",
    "    &nbsp;\n",
    "  <figcaption><i>Dieses Bild wurde übrigens von einer KI erzeugt.</i></figcaption>\n",
    "</figure> \n",
    "\n",
    "Zum Einstieg in diese Einheit rufen wir uns den Aufbau des Perzeptrons in Erinnerung. Das Perzeptron besteht aus einer festen Anzahl Inputs (abhängig von den Dimensionen der Punkte, die als Datengrundlage dienen), Gewichten mit denen die Eingaben mulipliziert und zusammen mit dem Bias addiert werden und einer Aktivierungsfunktion. Diesen Aufbau bezeichnen wir im Folgenden als <b>Neuron</b>.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/perzeptron.png\" alt=\"perzeptron\" style=\"width:70%\">\n",
    "</figure> \n",
    "\n",
    "\n",
    "## Aufbau neuronaler Netze\n",
    "\n",
    "Im Folgenden ändern wir das Perzeptron Schritt für Schritt ab, um dessen Defizite zu beheben.\n",
    "\n",
    "### Mehr als zwei Klassen klassifizieren und Performance steigern\n",
    "\n",
    "Um die Performance unserer KI zu steigern, schalten wir mehrere Neuronen hinter- und nebeneinander. Die Ausgabe eines Neurons dient nun als Eingabe von nachfolgenden Neuronen. Sind Neuronen parallel in einer Ebene angeordnet, wird die Gesamtheit dieser Neuronen als <b>Layer</b> (bzw. Schicht) bezeichnet. Das gesamte Konstrukt mehreren Neuronenschichten bezeichnet man als <b>neuronales Netz</b>. Wenn es mehrere verdeckte Schichten gibt, bezeichnet man das Netz als <b>tiefes neuronales Netz</b> (deep neural network).\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/nn1.png\" alt=\"perzeptron\" style=\"width:60%\">\n",
    "</figure> \n",
    "\n",
    "Um nicht nur zwei Klassen von Datenpunkten klassifizieren zu können, wird die Ausgabe durch mehrere Neuronen erweitert. Die Nummer des Neurons, das den größten Wert in der Ausgabeschicht ausgibt, ist auch die Ausgabe des gesamten neuronalen Netzes. Wenn es also fünf Ausgabeneuronen gibt und das mittlere den größten Wert hat, dann weist das neuronale Netz den Datenpunkt der Klasse 2 zu (Outputs 0 bis 5). Bisher sind die Ausgaben der Neuronen allerdings entweder 0 oder 1, so dass es oft zu einem Gleichstand kommen kann. Nicht nur deswegen sollten wir die bisherige Aktivierungsfunktion durch eine geeignetere ersetzen.\n",
    "\n",
    "### Neue Aktivierungsfunktion\n",
    "\n",
    "Das Perzeptron kann nur dann Datenpunkte verschiedener Klassen voneinander trennen, wenn die Datenpunkte der unterschiedlichen Klassen durch eine Gerade getrennt werden können. Das wird u.a. durch die Treppenfunktion verursacht, die wir als Aktivierungsfunktion verwenden. Außerdem gehen durch die Weiterleitung von entweder 0 oder 1 viele Informationen verloren, weil es keine Werte dazwischen gibt. Die <b>Sigmoidfunktion</b> $sig$ oder die <b>ReLU-Funktion</b> $relu$ sind in vielen Fällen besser als Aktivierungsfunktionen der Neuronen geeignet. Für unsere neuronalen Netze werden wir hauptsächlich die ReLU-Funktion verwenden.\n",
    "\n",
    "$$ sig(x) = \\dfrac{e^x}{e^x + 1} $$\n",
    "\n",
    "\n",
    "$$ relu(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0, & x \\leq 0 \\\\\n",
    "x, & \\, \\textrm{sonst} \\\\\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/sigmoid_and_relu.png\" alt=\"Sigmoid and ReLU\" style=\"width:50%\">\n",
    "</figure> \n",
    "\n",
    "### Softmax\n",
    "\n",
    "Jetzt fehlt nur noch eine kleine Änderung, um ein herkömmliches neuronales Netz zu erhalten. Wie im vorletzten Abschnitt bereits umrissen, wird die Klassifikation des Datenpunkts jetzt nicht mehr durch eine 0- oder 1-Ausgabe des letzten Neurons ermittelt, sondern durch die Nummer des Neurons in der Ausgabeschicht, das die größte Ausgabe hat. Durch die neue ReLU-Aktivierungsfunktion erhalten wir in der letzten Ausgabeschicht nicht mehr 0- oder 1-Ausgaben, sondern Werte größer oder gleich 0. \n",
    "Um als Ausgabe des neuronalen Netzes die Wahrscheinlichkeit zu erhalten, mit der ein Datenpunkt einer Klasse zugeordnet wird, wird eine am Ende eine zusätzliche Schicht mit einer speziellen Aktivierungsfunktion (Softmax-Funktion) eingefügt, deren Gewichte nicht trainiert werden.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/nn2.png\" alt=\"perzeptron\" style=\"width:80%\">\n",
    "</figure> \n",
    "\n",
    "Jetzt sind wir bereit unser erstes neuronales Netz in Code umzusetzen. Damit wir nicht alles selbst implementieren müssen, verwenden wir die Bibliothek <i>PyTorch</i>, die von einem Facebook-Forschungsteam entwickelt wurde.\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "PyTorch bietet eine sehr einfache Weise, neuronale Netze zu konstruieren. Gehe das folgende Codefeld durch und führe es aus, um mit den Funktionsaufrufen vertraut zu werden. Wir konstruieren dabei das obige neuronale Netze mit vier Eingabe- und drei Ausgabeneuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Im Konstruktor werden die unterschiedlichen Schichten definiert\n",
    "    def __init__(self, num_in, num_out):\n",
    "        \n",
    "        # Der Konstruktur der Elternklasse muss aufgerufen werden.\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.name_model = \"Netzi\"\n",
    "        \n",
    "        # Durch den folgenden Funktionsaufruf wird eine Schicht mit num_in eingehenden \n",
    "        # und 5 ausgehenden Verbindungen konstruiert.\n",
    "        # Den Namen der Schichten kannst du selbst festlegen.\n",
    "        # fc steht für fully connected.\n",
    "        self.fc1 = nn.Linear(num_in, 5)\n",
    "        # Achte darauf, dass die folgende Schicht die Anzahl der eingehenden Verbindungen\n",
    "        # mit den ausgehenden Verbindungen der letzten Schicht übereinstimmt.\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        # Standardmäßig wird zu jedem Neuron ein Bias hinzugefügt. Durch den Parameter\n",
    "        # 'bias' kann das deaktiviert werden.\n",
    "        self.fc3 = nn.Linear(5, num_out, bias=False)\n",
    "        \n",
    "        # ReLU-Funktion\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        # Softmax-Funktion\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    # In dieser Funktion muss festgelegt werden, wie die Eingabe durch das Netz propagiert wird (d.h. durch\n",
    "    # die einzelnen Schichten „weitergereicht“ wird).\n",
    "    def forward(self, x):\n",
    "        # Zunächst wird die Eingabe mit den Gewichten der ersten Schicht multipliziert, \n",
    "        # in den einzelnen Neuronen aufsummiert und anschließend in die ReLU-Funktion eingesetzt.\n",
    "        output = self.relu(self.fc1(x))\n",
    "        # Die verarbeitete Eingabe wird nun durch die zweite Schicht propagiert. \n",
    "        output = self.relu(self.fc2(output))\n",
    "        # In der vorletzten Schicht gibt es keine ReLU-Funktion mehr.\n",
    "        output = self.fc3(output)\n",
    "        # finale Ausgabe des neuronalen Netzes\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "# Erzeugung eines Objekts des neuronalen Netzes\n",
    "erstes_nn = Net(4,3)\n",
    "print(f\"Hallo mein Name ist {erstes_nn.name_model}!\\n\")\n",
    "print(\"Das ist mein Aufbau:\\n\")\n",
    "print(erstes_nn, \"\\n\")\n",
    "print(\"Und das sind meine zufällig initialisierten Gewichte:\")\n",
    "for param in erstes_nn.named_parameters():\n",
    "    print(\"\\n\", param)\n",
    "\n",
    "# Das ist eine Testeingabe\n",
    "test_eingabe = torch.tensor([1.0, 2.5, -1, 0])\n",
    "# Die Ausgabe erhälst du entweder so\n",
    "print(\"\\nAusgabe:\", erstes_nn(test_eingabe))\n",
    "# oder durch den Funktionsaufruf forward(eingabe)\n",
    "print(\"Ausgabe:\", erstes_nn.forward(test_eingabe))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/img/laptop_icon.png\" width=50 height=50 /> <br><br>\n",
    "\n",
    "<i>Im letzten Codefeld wurden unser erstes neuronales Netz erzeugt. Lies die gesuchten Gewichte anhand der letzten Ausgabe ab und überprüfe deine Eingabe, indem du das Codefeld ausführst. Runde gegebenenfalls die Eingaben auf die vierte Nachkommastelle ab.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.code.help_functions import pruefe_gewichte\n",
    "\n",
    "# Ersetze die Nullen durch die richtigen Werte.\n",
    "\n",
    "# Gewicht zwischen dem ersten Neuron der Eingabeschicht und dem ersten Neuron der ersten verdeckten Schicht\n",
    "gewicht1 = 0\n",
    "\n",
    "# Bias des letzen Neurons der ersten verdeckten Schicht\n",
    "gewicht2 = 0\n",
    "\n",
    "# Bias des zweiten Neurons der zweiten verdeckten Schicht\n",
    "gewicht3 = 0\n",
    "\n",
    "# Gewicht zwischen dem vierten Neuron der zweiten verdeckten Schicht \n",
    "# und dem dritten Neuron der dritten verdeckten Schicht\n",
    "gewicht4 = 0\n",
    "\n",
    "print(pruefe_gewichte(erstes_nn, gewicht1, gewicht2, gewicht3, gewicht4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/img/laptop_icon.png\" width=50 height=50 /> <br><br>\n",
    "\n",
    "<i>Jetzt bist du bereit ein neuronales Netz eigenständig zu konstruieren. Implementiere das abgebildete neuronale Netz und gib das Ergebnis des durchpropagierten Datenpunkts an.</i>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/nn3.png\" alt=\"neuronales Netz\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datenpunkt = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Füge hier deinen Code ein."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis jetzt haben wir zwar neuronale Netze konstruiert, aber sie noch nicht trainieren lassen. Die vorhandenen Trainingsdaten müssen wir nutzen, um die Gewichte so anzupassen, dass das neuronale Netz auf den Testdaten (die wir nicht für das Training benutzen) gute Ergebnisse erzielt. Im nächsten Abschnitt schauen wir uns an, wie das funktioniert.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Der Algorithmus, der die Gewichte der neuronalen Netze abändert und ein entscheidender Faktor am Erfolg von Deep-Learning-Algorithmen ist, ist der <b>Backpropagation-Algorithmus</b>. Der Backpropagation-Algorithmus ist ein Optimierungsalgorithmus, d.h. bei der Funktion, die den Fehler des neuronalen Netzes beschreibt, wird (in diesem Fall) nach dem Minimum gesucht, weil wir den Fehler so klein wie möglich halten möchten. \n",
    "\n",
    "Die Suche nach dem Minimum können wir uns mit folgendem Bild veranschaulichen. Ein Weihnachtsmann sitzt in seinem E-Schlitten auf einem Hügel und möchte den Weg ins Tal finden. Leider kennt er den Weg dahin nicht. Zu allem Überfluss ist es auch schon dunkel und sogar etwas nebelig ist, sodass er nur zehn Meter weit sehen kann. Er kann aber um sich herum erkennen, in welche Richtung der Hügel am steilsten abfällt. (In diese Richtung zeigt übrigens auch die Ableitung der Funktion, die das Gelände beschreibt.) Er stellt sein E-Schlitten so ein, dass er eine bestimmte Distanz in die Richtung des steilsten Abstiegs fährt, anschließend stoppt, die Richtung des Abstiegs noch einmal neu bestimmt und in diese Richtung wieder eine bestimmte Distanz fährt. Wenn alles optimal verläuft, findet er auf diese Weise den Weg ins Tal.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/loss_function.png\" alt=\"Verlustfunktion\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "Analog dazu funktioniert auch die Optimierung bei neuronalen Netzen. Die Funktion, deren globales Minimum erreicht werden soll, heißt <b>Verlustfunktion / Loss-Funktion </b>. Die Funktion MSE (Mean Squared Error) ist ein Beispiel für so eine Funktion:\n",
    "\n",
    "$$MSE = \\dfrac{1}{n} \\bigl[ (y_1 - o_1)^2 + (y_2 - o_2)^2 + \\dots + (y_n - o_n)^2 \\bigr], $$\n",
    "\n",
    "wobei $(y_1, \\dots, y_n)$ die optimale und $(o_1, \\dots, o_n)$ die tatsächliche Ausgabe eines neuronalen Netzes beschreibt. \n",
    "\n",
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "    \n",
    "<i>Wenn wir z.B. einen Datenpunkt betrachten, der ein Blaumeisen-Ei repräsentiert, dann ist die optimale Ausgabe bei drei möglichen Klassen (Klasse 0 = Blaumeisen, Klasse 1 = Ente, Klasse 2 = Greifvogel) der Vektor $(1, 0, 0)$. Wenn die tatsächliche Ausgabe des neuronalen Netzes $(0.5, 0.25, 0.25)$ ist, was ist dann der Verlust nach der oberen Formel?</i>\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>➤ Klicke hier, um deine Antwort zu prüfen.</summary>\n",
    "   \n",
    "$$\\dfrac{1}{3} \\bigl[ (1 - 0.5)^2 + (0 - 0.25)^2 + (0 - 0.25)^2 \\bigr] = 0.375.$$\n",
    "    \n",
    "Wenn das neuronale Netz nur ein Gewicht hat, könnte die Verlustfunktion so aussehen:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/loss_function2.png\" alt=\"Verlustfunktion\" style=\"width:45%\">\n",
    "</figure> \n",
    "\n",
    "Das aktuelle Gewicht $w_1$ von $0.7$ muss also ein bisschen vergrößert werden, um den Verlust zu verkleinern.\n",
    "   \n",
    "</details>\n",
    "\n",
    "Wenn das neuronale Netz nur zwei Gewichte hat, könnte eine Verlustfunktion wie folgt aussehen. Bei mehr als zwei Gewichten (in der Praxis eingesetzte neuronale Netze haben Millionen von trainierbaren Gewichten) ist eine Visualisierung allerdings nicht mehr so einfach möglich.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/train_val_loss_landscape.png\" alt=\"Loss-Function\" style=\"width:50%\">\n",
    "</figure> \n",
    "\n",
    "Wenn wir bestimmt haben, ob wir ein Gewicht verkleinern oder vergrößern müssen, um den Verlust zu reduzieren, müssen wir noch festlegen, wie stark wir das Gewicht verändern möchten. Dabei können unterschiedliche Probleme auftreten. Ist die Veränderung des Gewichts zu gering, kann es sein, dass das neuronale Netz in einem lokalen Minimum stecken bleibt oder sich nur sehr langsam dem globalen Minimum nähert. Verändern wir das Gewicht zu stark, ist es möglich, dass wir über das Ziel hinausschießen. \n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/loss_function3.png\" alt=\"Verlustfunktion\" style=\"width:95%\">\n",
    "</figure> \n",
    "\n",
    "Wir müssen also die <b>Lernrate</b> des neuronalen Netzes mit Bedacht wählen und möglicherweise immer wieder anpassen. Die Update-Regel für jedes Gewicht $w$ im neuronalen Netz können wir folgendermaßen notieren:\n",
    "\n",
    "$$w_{\\text{neu}} \\longleftarrow w_{\\text{alt}} - \\alpha \\cdot \\Delta w.$$\n",
    "\n",
    "$\\alpha$ ist die Lernrate und $\\Delta w$ der Gradient (die Ableitung) des Gewichts. Der Gradient gibt nicht nur die Richtung an, in der das Gewicht verändert werden muss, sondern beschreibt auch, wie stark das betrachtete Gewicht zu dem Verlust beigetragen hat. \n",
    "\n",
    "Den Gradienten eines Gewichts $w$ bestimmen wir, indem wir die Verlustfunktion nach $w$ durch mehrfache Anwendung der Kettenregel ableiten. Da dieser Prozess sehr mühselig ist, verzichten wir an dieser Stelle auf weitere Details, weil PyTorch für uns diese Arbeit übernehmen wird.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"resources/img/backpropagation.png\" alt=\"Verlustfunktion\" style=\"width:65%\">\n",
    "</figure> \n",
    "\n",
    "Die Berechnung der Gradienten bei der Backpropagation erfordert sehr viel Rechenaufwand. Eine CPU wird nur bei kleinen Daten(mengen) gute Ergebnisse in überschaubarer Zeit liefern können. Aus diesem Grund verwendet man GPU-Einheiten (Grafikprozessoren), um ein neuronales Netz trainieren zu lassen. Der Vorteil dieser Verwendung besteht darin, dass die Berechnungen <i>parallel</i> ablaufen können und das Netz somit viel schneller trainiert.\n",
    "\n",
    "## Training eines neuronalen Netzes\n",
    "\n",
    "Nach so viel Theorie können wir endlich neuronale Netze trainieren lassen! Untersuche den Code, um dein eigenes neuronales Netz weiter unten an die Daten anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.code.help_functions import daten, datenpunkte_zeichnen\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "torch.manual_seed(1)  \n",
    "\n",
    "x_train, y_train, x_test, y_test = daten()\n",
    "print(f\"Wir haben {len(y_train)} Trainingsdatenpunkte und {len(y_test)} Testdatenpunkte zur Verfügung.\")\n",
    "datenpunkte_zeichnen(x_train, y_train, ['#ec90cc', '#4f7087'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir implementieren folgendes neuronales Netz, das du bereits oben konstruiert hast.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn3.png\" alt=\"neuronales Netz\" style=\"width:60%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_in, num_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_in, 2)\n",
    "        self.fc2 = nn.Linear(2, 2)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit dieser Funktion wird die Genauigkeit von \n",
    "# einem neuronalen Netz auf einem Datensatz gemessen.\n",
    "\n",
    "'''\n",
    "@param model: neuronales Netz auf dem die Messung durchgeführt wird\n",
    "@param x: Datenpunkte \n",
    "@param labels: Labels zu den Datenpunkten\n",
    "@param name: Name des Datensatzes z.B. Training oder Test\n",
    "'''\n",
    "def evaluation(model, x, labels):\n",
    "    model.train(False)\n",
    "    outputs = model(x)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_func(outputs, labels)\n",
    "    correct = sum(torch.eq(preds, labels)).item()\n",
    "    total = len(labels)\n",
    "    accuracy = round(correct/total, 3) * 100\n",
    "    return round(loss.item(), 5), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier wird das neuronale Netz mit zwei Eingabe- und zwei Ausgabeneuronen erzeugt.\n",
    "net = Net(2,2)\n",
    "\n",
    "# Der Optimizer ist dafür zuständig die Gradienten zu berechnen und die Lernrate \n",
    "# zu steuern. \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# Als Loss-Funktion wählen wir die gebräuchlichste Kreuzentropie-Funktion. \n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier evaluieren wir die Genauigkeit des untrainierten neuronalen Netzes.\n",
    "loss_res, accuracy_res = evaluation(net, x_test, y_test)\n",
    "print(f\"Untrainiertes neuronales Netz - Ergebniss für Trainingsdatensatz: Genauigkeit={accuracy_res}%, Loss={loss_res}\")\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    ''' Trainingsprozess '''\n",
    "    \n",
    "    # Das neuronale Netz wird in den Trainingsmodus versetzt (d.h. es werden Gradienten berechnet).\n",
    "    net.train(True)\n",
    "    \n",
    "    # Alle Trainingspunkte werden durch das neuronale Netz propagiert. \n",
    "    outputs = net(x_train)\n",
    "    \n",
    "    # Der Loss hängt von der Ausgabe des neuronalen Netzes und den tatsächlichen Labeln ab.\n",
    "    loss = loss_func(outputs, y_train)\n",
    "    \n",
    "    # Alle berechneten Gradienten vom letzten Durchgang werden gelöscht.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Mit diesem Funktionsaufruf werden die Gradienten berechnet.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Hier wird für jedes Gewicht ein Update durchgeführt.\n",
    "    optimizer.step()\n",
    "    \n",
    "    ''' Evaluation auf den Trainings- und Testdaten '''\n",
    "    \n",
    "    # Evaluation wird für jede 5. Epoche durchgeführt. \n",
    "    if e % 5 == 0:\n",
    "        loss_res, accuracy_res = evaluation(net, x_train, y_train)\n",
    "        print(f\"{e}. Epoche - Ergebniss für Trainingsdatensatz: Genauigkeit={accuracy_res}%, Loss={loss_res}\")\n",
    "        loss_res, accuracy_res = evaluation(net, x_test, y_test)\n",
    "        print(f\"{e}. Epoche - Ergebniss für Testdatensatz: Genauigkeit={accuracy_res}%, Loss={loss_res}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img style=\"float: left;\" src=\"resources/img/laptop_icon.png\" width=50 height=50 /> <br><br>\n",
    "\n",
    "<i>Du kennst nun alle Codebausteine, um dein eigenes neuronales Netz zu konstruieren und es trainieren zu lassen. Setze ein neuronales Netz für die folgenden Daten um und passe die Gewichte an den Datensatz an. Brich das Training ab, sobald das Netz eine 93%-Genauigkeit auf dem Trainingsdatensatz erzielt. Speichere außerdem in jeder Epoche das Netz, das über alle vergangenen Durchläufe hinweg die höchste Genauigkeit erreicht hat.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.code.help_functions import daten2, datenpunkte_zeichnen\n",
    "\n",
    "x_train, y_train, x_test, y_test = daten2()\n",
    "print(f\"Wir haben {len(y_train)} Trainingsdatenpunkte und {len(y_test)} Testdatenpunkte zur Verfügung.\")\n",
    "datenpunkte_zeichnen(x_train, y_train, ['#ec90cc', '#8b4513', '#4f7087'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementiere hier die Klasse für dein neuronales Netz.\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialisiere die Gewichtsmatrizen für die beiden Schichten\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Berechne die Ausgabe des neuronalen Netzes für die gegebenen Eingaben\n",
    "        hidden = np.dot(inputs, self.weights_input_hidden)\n",
    "        hidden_activated = self._sigmoid(hidden)\n",
    "        output = np.dot(hidden_activated, self.weights_hidden_output)\n",
    "        return self._softmax(output)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        # Sigmoid-Aktivierungsfunktion\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        # Softmax-Aktivierungsfunktion\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    def train(self, x_train, y_train, epochs, learning_rate):\n",
    "        highest_accuracy = 0\n",
    "        best_weights_input_hidden = None\n",
    "        best_weights_hidden_output = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Vorwärtspropagation\n",
    "            hidden = np.dot(x_train, self.weights_input_hidden)\n",
    "            hidden_activated = self._sigmoid(hidden)\n",
    "            output = np.dot(hidden_activated, self.weights_hidden_output)\n",
    "            output_activated = self._softmax(output)\n",
    "            \n",
    "            # Berechnung des Trainingsfehlers (Cross-Entropy-Loss)\n",
    "            loss = -np.sum(y_train * np.log(output_activated))\n",
    "            \n",
    "            # Berechnung der Genauigkeit\n",
    "            predictions = np.argmax(output_activated, axis=1)\n",
    "            true_labels = np.argmax(y_train, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            # Überprüfen, ob die aktuelle Genauigkeit die bisher beste ist\n",
    "            if accuracy > highest_accuracy:\n",
    "                highest_accuracy = accuracy\n",
    "                best_weights_input_hidden = np.copy(self.weights_input_hidden)\n",
    "                best_weights_hidden_output = np.copy(self.weights_hidden_output)\n",
    "            \n",
    "            # Abbruchbedingung, wenn die gewünschte Genauigkeit erreicht ist\n",
    "            if accuracy >= 0.93:\n",
    "                print(\"Training abgebrochen - Zielgenauigkeit erreicht.\")\n",
    "                break\n",
    "            \n",
    "            # Rückwärtspropagation und Gewichtsaktualisierung\n",
    "            output_error = output_activated - y_train\n",
    "            hidden_error = np.dot(output_error, self.weights_hidden_output.T) * (hidden_activated * (1 - hidden_activated))\n",
    "            \n",
    "            self.weights_hidden_output -= learning_rate * np.dot(hidden_activated.T, output_error)\n",
    "            self.weights_input_hidden -= learning_rate * np.dot(x_train.T, hidden_error)\n",
    "        \n",
    "        # Wähle die besten Gewichte aus, die über alle Epochen die höchste Genauigkeit erzielt haben\n",
    "        self.weights_input_hidden = best_weights_input_hidden\n",
    "        self.weights_hidden_output = best_weights_hidden_output\n",
    "\n",
    "# Annahme: x_train und y_train sind Ihre Trainingsdaten\n",
    "# Verwende die gewünschte Anzahl von Neuronen in der versteckten Schicht (z.B. 100)\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 100\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Erstelle ein neuronales Netz\n",
    "neural_net = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Starte das Training\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "neural_net.train(x_train, y_train, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeuge hier das Objekt deiner Klasse und den Optimizer. \n",
    "# Lege hier außerdem deine Loss-Funktion und die Anzahl der Epochen fest.\n",
    "from resources.code.help_functions import daten2, datenpunkte_zeichnen\n",
    "\n",
    "# Daten laden\n",
    "x_train, y_train, x_test, y_test = daten2()\n",
    "print(f\"Wir haben {len(y_train)} Trainingsdatenpunkte und {len(y_test)} Testdatenpunkte zur Verfügung.\")\n",
    "datenpunkte_zeichnen(x_train, y_train, ['#ec90cc', '#8b4513', '#4f7087'])\n",
    "\n",
    "# Erzeuge das Objekt deiner Klasse und den Optimizer.\n",
    "# Annahme: x_train und y_train sind Ihre Trainingsdaten\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 100\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Erstelle ein neuronales Netz\n",
    "neural_net = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Wähle den Optimizer (z.B. Gradientenabstieg)\n",
    "# Beachte, dass Sie verschiedene Optimizer verwenden können, z.B. Adam, RMSprop usw.\n",
    "optimizer = \"gradient_descent\"\n",
    "\n",
    "# Lege die Loss-Funktion fest (z.B. Cross-Entropy-Loss)\n",
    "loss_function = \"cross_entropy\"\n",
    "\n",
    "# Lege die Anzahl der Epochen fest\n",
    "epochs = 1000\n",
    "\n",
    "# Führe das Training durch\n",
    "learning_rate = 0.1\n",
    "neural_net.train(x_train, y_train, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementiere hier deinen Trainingsprozess.\n",
    "# Breche den Trainingsprozess ab, wenn eine Genauigkeit von 93% auf den\n",
    "# Trainingsdaten erreicht wurde.\n",
    "# Speicher außerdem immer das bisher beste Model mit deepcopy(model) \n",
    "# in einer Variablen ab.\n",
    "\n",
    "from copy import deepcopy\n",
    "from copy import deepcopy\n",
    "\n",
    "# Annahme: x_train und y_train sind Ihre Trainingsdaten\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 100\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "# Erstelle ein neuronales Netz\n",
    "neural_net = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Wähle den Optimizer (z.B. Gradientenabstieg)\n",
    "# Beachte, dass Sie verschiedene Optimizer verwenden können, z.B. Adam, RMSprop usw.\n",
    "optimizer = \"gradient_descent\"\n",
    "\n",
    "# Lege die Loss-Funktion fest (z.B. Cross-Entropy-Loss)\n",
    "loss_function = \"cross_entropy\"\n",
    "\n",
    "# Lege die Anzahl der Epochen fest\n",
    "epochs = 1000\n",
    "\n",
    "# Führe das Training durch\n",
    "learning_rate = 0.1\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Vorwärtspropagation\n",
    "    output_activated = neural_net.forward(x_train)\n",
    "    \n",
    "    # Berechnung des Trainingsfehlers (Cross-Entropy-Loss)\n",
    "    loss = -np.sum(y_train * np.log(output_activated))\n",
    "    \n",
    "    # Berechnung der Genauigkeit\n",
    "    predictions = np.argmax(output_activated, axis=1)\n",
    "    true_labels = np.argmax(y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Überprüfen, ob die aktuelle Genauigkeit die bisher beste ist\n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_model = deepcopy(neural_net)\n",
    "    \n",
    "    # Abbruchbedingung, wenn die gewünschte Genauigkeit erreicht\n",
    "    if accuracy >= 0.93:\n",
    "        print(\"Training abgebrochen - Zielgenauigkeit erreicht.\")\n",
    "        break\n",
    "    \n",
    "    # Rückwärtspropagation und Gewichtsaktualisierung\n",
    "    output_error = output_activated - y_train\n",
    "    hidden_error = np.dot(output_error, neural_net.weights_hidden_output.T) * (hidden_activated * (1 - hidden_activated))\n",
    "    \n",
    "    neural_net.weights_hidden_output -= learning_rate * np.dot(hidden_activated.T, output_error)\n",
    "    neural_net.weights_input_hidden -= learning_rate * np.dot(x_train.T, hidden_error)\n",
    "\n",
    "# Das beste Modell wurde in der Variablen best_model gespeichert.\n",
    "# Ablauf fertig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Bildquellen</h2>\n",
    "\n",
    "https://pixabay.com/de/photos/ai-generiert-junge-junger-mann-7772478/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
