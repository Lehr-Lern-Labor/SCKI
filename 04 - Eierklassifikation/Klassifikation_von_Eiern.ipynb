{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf88e49",
   "metadata": {},
   "source": [
    "# Bilderklassifikation\n",
    "\n",
    "In der letzten Einheit haben wir gelernt, wie wir in PyTorch neuronale Netze implementieren und trainieren können. Als Datengrundlage dienten dabei zweidimensionale Punkte, die Vogeleier repräsentiert haben. Jeder dieser Dimensionen stand für eine Eigenschaft des Eis. So beschrieb der Datenpunkt $(8, 0.1)$ ein $8$ cm hohes und relativ dunkles Ei. Diese Klassifizierung setzt voraus, dass wir \n",
    "\n",
    "<ul>\n",
    "    <li>festlegen, welche Eigenschaften der zu klassifizierenden Objekte für eine Unterscheidungen zur anderen Art der Objekte relevant sind und</li>\n",
    "    <li>die festgelegten Eigenschaften für jedes Obejekt ausmessen.</li>\n",
    "</ul>\n",
    "\n",
    "Die Umsetzung beider Aspekte ist sehr schwierig und kostspielig. Es wäre viel praktischer, wenn wir Bilder von den verschiedenen Vogeleiern der KI zur Verfügung stellen und die KI die relevanten Eigenschaften der Objekte selbst herausfindet und ausmisst.\n",
    "\n",
    "Genau das möchten wir in dieser Einheit realisieren. Nach einem kurzen Theorieteil wirst du ein neuronales Netz implementieren, das Bilder klassifizieren kann.\n",
    "\n",
    "\n",
    "## Codierung von Bildern\n",
    "\n",
    "Farben werden im Computer als Zahlenwerte codiert. Die gebräuchlichste Codierungsmethode ist dabei die <b>RGB-Codierung</b>, bei der jede Farbe eine Mischung der drei Farbkanäle rot, grün und blau ist, deren Werte jeweils im Zahlenbereich 0 bis 255 liegen, also 8 Bit groß sind (bspw. kodiert das Tripel (255, 128, 0) die Farbe Orange). Für ein buntes Bild müssen pro Pixel folglich drei Zahlenwerte gespeichert werden. Um die Komplexität zu reduzieren, betrachten wir im Folgenden nur Graustufenbilder. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/lincoln_pixels.png\" alt=\"Abraham Lincoln Pixels\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Bei Graustufenbilder wird nur ein Zahlenwert pro Pixel gespeichert. Der Zahlenwert 0 entspricht einem komplett schwarzen Pixel und der Wert 255 einem weißen Pixel. Zahlenwerte zwischen 0 und 255 entsprechen unterschiedlichen Graustufen. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn_img.png\" alt=\"Bild in neuronales Netz\" style=\"width:70%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Bilder sind nichts anderes als zusammgesetzte Pixel und für den Computer somit einfach nur Listen aus Zahlen, die sich als Eingaben für neuronale Netze sehr gut eignen. Die Anzahl der Pixel muss dabei der Anzahl der Neuronen der Eingabeschicht entsprechen. Ein Bild, das nur aus einem Pixel besteht, können wir als Punkt in einem eindimensionalen Koordinatensystem auffassen. Ein Bild aus zwei Pixeln ist ein Punkt im zweidimensionalen Koordinatensytem usw. Auch Bilder können demzufolge als $n$-dimensionale Punkte aufgefasst werden, die wir uns allerdings nicht mehr wirklich vorstellen können.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/nn_klassen.png\" alt=\"Klassen\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Genauso wie das Perzeptron trennen neuronale Netze eigentlich nur Datenpunkte voneinander durch Grenzen, die sie selbst ziehen. Diese Grenzen sind keine Geraden oder Ebenen wie Perzeptron, sondern gekrümmte $n$-dimensionale Linien. Mit Hilfe der Trainingsdaten lernen neuronale Netze den (vermeintlichen) Verlauf dieser Grenzen, sodass sie anschließend ungesehene Daten den unterschiedlichen Klassen zuordnen. In der oberen Abbildung ist zu sehen wie das neuronale Netz nach dem Anpassen der Gewichte an die Trainingsdaten den roten, unbekannten Datenpunkt der Klasse 3 zuweist.\n",
    "\n",
    "## Vogeleierklassifikation\n",
    "\n",
    "In der letzten Einheit hast du (fast) alle notwendigen Funktionsaufrufe in Python kennen gelernt, um jetzt ein eigenes neuronales Netz zu konstruieren, das Bilder klassifizieren kann. Bei den Bildern handelt es sich um Vogeleier, die du und deine Mitschüler:innen gemalt haben. Dein neuronales Netz wird nach dem Training in der Lage sein, Blaumeisen-, Enten- und Greifvögeleier auseinanderhalten zu können. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/vogeleier.jpg\" alt=\"Vogeleier\" style=\"width:70%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "____\n",
    "\n",
    "<i class=\"fa fa-laptop\" style=\"font-size:38px\"></i>\n",
    "\n",
    "<i>Ergänze die folgenden Codefeldern den Kommentaren entsprechend, um dein eigenes neuronales Netz zu konstruieren, das die Bilder der Vogeleier richtig klassifizieren kann.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb4192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Füge hier den Pfad zu deinen Trainings- und Testdaten ein.\n",
    "\n",
    "TRAIN_DATA_PATH = 'DataSet/train'\n",
    "TEST_DATA_PATH = 'DataSet/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d0fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from resources.code.help_functions import ei_zeichnen\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "  transforms.Resize([64,64]),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Grayscale()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DATA_PATH, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "ei_zeichnen(train_dataset[105][0], f\"Klasse {train_dataset[105][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746916d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementiere in diesem Feld dein neuronales Netz.\n",
    "# Wähle für die Bilder dabei eine passende Architektur.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, num_in, num_out):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4096, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 64)\n",
    "        self.fc5 = nn.Linear(64, num_out)\n",
    "        \n",
    "\n",
    "        self.relu=torch.nn.ReLU()\n",
    "\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.relu(self.fc1(x))\n",
    "        output = self.relu(self.fc2(output))\n",
    "        output = self.relu(self.fc3(output))\n",
    "        output = self.relu(self.fc4(output))\n",
    "        output = self.fc5(output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "net = Net(4096, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f95e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "995c1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# print(loss.item())\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "for x,y in train_dataset:\n",
    "    total += 1\n",
    "    output = net(x.view(-1))\n",
    "    output = torch.unsqueeze(output, 0)\n",
    "    if torch.argmax(output).item() == y:\n",
    "        count += 1\n",
    "\n",
    "print(count, total)\n",
    "for _ in tqdm(range(5)):\n",
    "    for x,y in train_dataset:\n",
    "        y = torch.tensor([y])\n",
    "        output = net(x.view(-1))\n",
    "        output = torch.unsqueeze(output, 0)\n",
    "        loss = loss_func(output, y)\n",
    "        # print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "count = 0 \n",
    "total = 0\n",
    "for x,y in train_dataset:\n",
    "    total += 1\n",
    "    output = net(x.view(-1))\n",
    "    output = torch.unsqueeze(output, 0)\n",
    "    if torch.argmax(output).item() == y:\n",
    "        count += 1\n",
    "        \n",
    "print(count, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43f94f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# x, y = x.long(), y.long()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for _ in range(10):\n",
    "    for x, y in train_loader:\n",
    "        # x, y = x.long(), y.long()\n",
    "        outputs = net(x.view(x.shape[0],1,-1))\n",
    "       \n",
    "        loss = loss_func(torch.argmax(outputs, dim=2), y)\n",
    "        print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea65f9",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "\n",
    " <figure>\n",
    "  <img src=\"resources/img/overfitting.png\" alt=\"Overfitting\" style=\"width:50%\">\n",
    "  <figcaption></figcaption>\n",
    "</figure> \n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c81dee",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<i style=\"font-size:38px\">?</i>\n",
    "\n",
    "    \n",
    "<i>Wir haben gelernt, wie wir das Perzeptron trainieren können Doch warum haben wir das eigentlich gemacht?</i>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>➤ Klicke hier, um deine Antwort zu prüfen.</summary>\n",
    "   \n",
    "Wir möchten das Perzeptron so trainieren, dass wir <b>unbekannte</b> Daten, auf denen wir unser Perzeptron <b>nicht</b> trainiert haben, richtig klassifizieren. Die Performance auf den <b>Trainingsdaten</b> dient zwar als Orientierung, ist aber nicht besonders wichtig.\n",
    "   \n",
    "</details>\n",
    "\n",
    "\n",
    "<h3>Aktuelle Forschung</h3>\n",
    "\n",
    "Die Forschung zu neuronalen Netzen ist noch lange nicht ausgeschöpft. Es gibt noch sehr vieles zu entdecken. So wissen wir z.B. immer noch nicht richtig, was genau neuronale Netze eigentlich lernen. \n",
    "\n",
    "Wenn du dich für die aktuelle Forschung zu neuronalen Netzen oder KI interessiert, ist folgender YouTube-Kanal empfehlenswert: \n",
    "\n",
    "https://www.youtube.com/c/K%C3%A1rolyZsolnai/videos?view=0&sort=p&shelf_id=0"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
